#Neural netork is like brain neuron of firing on and off.
#Input and output.
#Each neuron has a weight and a value. The output has the value value_1 * weight_1 +.... value_n * weight_n -> weighted sum.
#Have to incorporate bias as well.
#Adjust weight and bias to adjust to correct answer.
#Adding bunch of mx+b so kind of like linear regression.
#Activation function: adds a degree of complexity so it doesn't always have to be linear.
#Rectified linear thing: negative values become 0 and remaining positive more positive to prevent negative range.
